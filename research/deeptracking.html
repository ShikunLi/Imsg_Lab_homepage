<!-- <script src="http://www.google.com/jsapi" type="text/javascript"></script> -->
<!-- <script type="text/javascript">google.load("jquery", "1.3.2");</script> -->

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    p {
        text-align: justify;
    }
</style>

<html>


<head>
    <link rel="icon" href="../favicon.png" type="image/x-icon">
    <title>Deep Visual Tracking</title>
    <!-- <meta property="og:image" content="./asset/splash.png" /> -->
    <meta property="og:title" content="Deep Visual Tracking" />
</head>

<body>
    <br>
    <center>
        <span style="font-size:26px" id="1"><strong>Accurate UAV Tracking with Distance-Injected Overlap
                Maximization</strong></span><br>
        <br><br>
        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#">Chunhui Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Kangkai Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dan Zeng</a></span> &emsp;
                </td>
            </tr>
        </table>

        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://doi.org/10.1145/3394171.3413959'> Paper [
                            ACM MM 2020]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/UAVTracking20.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            UAV tracking is usually challenged by the dual-dynamic disturbances that arise from not only diverse moving
            target but also motion camera, leading to a more serious model drift issue than traditional visual tracking.
            In this work, we propose to alleviate this issue with distance-injected overlap maximization. Our idea is
            improving the accuracy of target localization by deriving a conceptually simple target localization loss and
            a global feature recalibration scheme in a mutual reinforced way. In particular, the target localization
            loss is designed by simply incorporating the normalized distance of target offset and generic semantic IoU
            loss, resulting in the distance-injected semantic IoU loss, and its minimal solution can alleviate the drift
            problem caused by camera motion. Moreover, the deep feature extractor is reconstructed and alternated with a
            feature recalibration network, which can leverage the global information to recalibrate significant features
            and suppress negligible features. Following by multi-scale feature concat, the proposed tracker can improve
            the discriminative capability of feature representation for UAV targets on the fly. Extensive experimental
            results on four benchmarks, i.e. UAV123, UAVDT, DTB70, and VisDrone, demonstrate the superiority of the
            proposed tracker against existing state-of-the-arts on UAV tracking.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Chunhui2020ACMMM,
  author = {Chunhui Zhang, and Shiming Ge, and Kangkai Zhang, and Dan Zeng},
  title = {Accurate UAV Tracking with Distance-Injected Overlap Maximization},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  pages={565â€“573},
  year = {2020},
}         
</span>
        </pre>
    </table>
    <hr>

    <br><br>

    <br>
    <center>
        <span style="font-size:26px" id="ge2020cascaded"><strong>Cascaded Correlation Refinement for Robust Deep
                Tracking</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Chunhui Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Shikun Li</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dan Zeng</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dacheng Tao</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/9069312'> Paper [IEEE
                            Trans]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/CascadedCorrelation20.webp" height="270px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Recent deep trackers have shown superior performance in visual tracking. In this article, we propose a
            cascaded correlation refinement approach to facilitate the robustness of deep tracking. The core idea is to
            address accurate target localization and reliable model update in a collaborative way. To this end, our
            approach cascades multiple stages of correlation refinement to progressively refine target localization.
            Thus, the localized object could be used to learn an accurate on-the-fly model for improving the reliability
            of model update. Meanwhile, we introduce an explicit measure to identify the tracking failure and then
            leverage a simple yet effective look-back scheme to adaptively incorporate the initial model and on-the-fly
            model to update the tracking model. As a result, the tracking model can be used to localize the target more
            accurately. Extensive experiments on OTB2013, OTB2015, VOT2016, VOT2018, UAV123, and GOT10k demonstrate that
            the proposed tracker achieves the best robustness against the state of the arts.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@ARTICLE{9069312,
    author={S. {Ge} and C. {Zhang} and S. {Li} and D. {Zeng} and D. {Tao}},
    journal={IEEE Transactions on Neural Networks and Learning Systems}, 
    title={Cascaded Correlation Refinement for Robust Deep Tracking}, 
    year={2021},
    volume={32},
    number={3},
    pages={1276-1288},
    doi={10.1109/TNNLS.2020.2984256}
}
</span>
        </pre>
    </table>
    <hr>
    <br><br>

    <br>
    <center>
        <span style="font-size:26px" id="ge2020distilling"><strong>Distilling Channels for Efficient Deep
                Tracking</strong></span><br>

        <br><br>
        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Zhao Luo</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Chunhui Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Yingying Hua</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dacheng Tao</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/8891903'> Paper [IEEE
                            Trans]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/DistillingChannels20.webp" height="300px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained
            deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep
            networks employed are usually trained to extract rich knowledge from massive data used in object
            classification and so they are capable to represent generic objects very well. However, these networks are
            too complex to represent a specific moving object, leading to poor generalization as well as high
            computational and memory costs. This paper presents a novel and general framework termed channel
            distillation to facilitate deep trackers. To validate the effectiveness of channel distillation, we take
            discriminative correlation filter (DCF) and ECO for example. We demonstrate that an integrated formulation
            can turn feature compression, response map generation, and model update into a unified energy minimization
            problem to adaptively select informative feature channels that improve the efficacy of tracking moving
            objects on the fly. Channel distillation can accurately extract good channels, alleviating the influence of
            noisy channels and generally reducing the number of channels, as well as adaptively generalizing to
            different channels and networks. The resulting deep tracker is accurate, fast, and has low memory
            requirements. Extensive experimental evaluations on popular benchmarks clearly demonstrate the effectiveness
            and generalizability of our framework.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@ARTICLE{8891903,
  author={S. {Ge} and Z. {Luo} and C. {Zhang} and Y. {Hua} and D. {Tao}},
  journal={IEEE Transactions on Image Processing}, 
  title={Distilling Channels for Efficient Deep Tracking}, 
  year={2020},
  volume={29},
  number={},
  pages={2610-2621},
}
</span>
        </pre>
    </table>
    <hr>

    <br>
    <center>
        <span style="font-size:26px" id="zhang2019robust"><strong>Robust Deep Tracking with Two-step Augmentation
                Discriminative Correlation
                Filters</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#">Chunhui Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Yingying Hua</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dan Zeng</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/8785041'>Paper [IEEE
                            Trans]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->

                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/RobustDeepTracking19.webp" height="320px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Recently, deep trackers have proven success in visual tracking due to their powerful feature representation.
            Among them, discriminative correlation filter (DCF) paradigm is widely used. However, these trackers are
            still difficult to learn an adaptive appearance model of the object due to the limited data available. To
            address that, this paper proposes a two-step augmentation discriminative correlation filters (TADCF)
            approach to improve robustness. Firstly, we propose an online frame augmentation scheme to obtain rich and
            robust deep features which can effectively alleviate background distractors, leading to better
            generalization and adaptation of the learned model. Secondly, an object augmentation mechanism is
            implemented by exploiting rotation continuity restriction, which simultaneously models target appearance
            changes from rotation and scale variations. Extensive experiments on four benchmarks illustrate that the
            proposed approach performs favorably against state-of-the-art trackers.
        </p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@INPROCEEDINGS{8785041,
  author={C. {Zhang} and S. {Ge} and Y. {Hua} and D. {Zeng}},
  booktitle={2019 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Robust Deep Tracking with Two-step Augmentation Discriminative Correlation Filters}, 
  year={2019},
  volume={},
  number={},
  pages={1774-1779},
}      
</span>
        </pre>
    </table>
    <hr>


</body>

</html>