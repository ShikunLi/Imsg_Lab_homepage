<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h3 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    p {
        text-align: justify;
    }
</style>

<html>

<head>
    <link rel="icon" href="../favicon.png" type="image/x-icon">

    <title>Controllable Representation Learning</title>
    <!-- <meta property="og:image" content="./asset/splash.png" /> -->
    <meta property="og:title" content="Controllable Representation Learning" />
</head>

<body>
    <br>
    <p>Representation Learning from Noisy-labeled Data</p>

    <center>
        <span style="font-size:26px" id="Li2022CVPR"><strong>Selective-Supervised Contrastive Learning with Noisy
                Labels</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#" target="_blank">Shikun Li</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xiaobo Xia</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html">Shiming Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Tongliang Liu</a></span> &emsp;
                </td>
            </tr>
        </table>

        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/9661404'> Paper [CVPR
                            2022]</a></span> &emsp;&emsp;
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="" height="300px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Li2022CVPR,
    author={Shikun Li and Xiaobo Xia and Shiming Ge and Tongliang Liu},
    booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Selective-Supervised Contrastive Learning with Noisy Labels}, 
    year={2022}} 
</span>
        </pre>
    </table>
    <hr>

    <p></p>
    <center>
        <span style="font-size:26px" id="9661404"><strong>Trustable Co-label Learning from Multiple Noisy
                Annotators</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#" target="_blank">Shikun Li</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Tongliang Liu</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Jiyong Tan</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dan Zeng</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html">Shiming Ge</a></span> &emsp;
                </td>
            </tr>
        </table>

        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/9661404'> Paper [TMM
                            2021]</a></span> &emsp;&emsp;
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="" height="300px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@ARTICLE{9661404,
    author={Li, Shikun and Liu, Tongliang and Tan, Jiyong and Zeng, Dan and Ge, Shiming},
    journal={IEEE Transactions on Multimedia}, 
    title={Trustable Co-label Learning from Multiple Noisy Annotators}, 
    year={2021},
    volume={},
    number={},
    pages={1-1},
    doi={10.1109/TMM.2021.3137752}}   
</span>
        </pre>
    </table>
    <hr>



    <center>
        <span style="font-size:26px" id="li2020coupled"><strong>Coupled-View Deep Classifier Learning from Multiple
                Noisy Annotators</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#" target="_blank">Shikun Li</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html">Shiming Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Yingying Hua</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Chunhui Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Hao Wen</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Tengfei Liu</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Weiqiang Wang</a></span> &emsp;
                </td>
            </tr>
        </table>

        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://aaai.org/ojs/index.php/AAAI/article/view/5898'> Paper
                            [AAAI 2020]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/li2020coupled.webp" height="300px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Typically, learning a deep classifier from massive cleanly annotated instances is effective but impractical
            in many real-
            world scenarios. An alternative is collecting and aggregating multiple noisy annotations for each instance
            to train the
            classifier. Inspired by that, this paper proposes to learn deep classifier from multiple noisy annotators
            via a coupled-view
            learning approach, where the learning view from data is represented by deep neural networks for data
            classification and
            the learning view from labels is described by a Naive Bayes classifier for label aggregation.Such
            coupled-view learning is
            converted to a supervised learning problem under the mutual supervision of the aggregated and predicted
            labels, and can be
            solved via alternate optimization to update labels and refine the classifiers. To alleviate the propagation
            of incorrect labels,
            small-loss metric is proposed to select reliable instances in both views. A co-teaching strategy with
            class-weighted loss
            is further leveraged in the deep classifier learning, which uses two networks with different learning
            abilities to teach each
            other, and the diverse errors introduced by noisy labels can be filtered out by peer networks. By these
            strategies, our
            approach can finally learn a robust data classifier which less overfits to label noise. Experimental results
            on synthetic and
            real data demonstrate the effectiveness and robustness of the proposed approach.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Li2020CVL,
  title     = {Coupled-view Deep Classifier Learning from Multiple Noisy Annotators},
  author    = {Shikun Li and Shiming Ge and Yingying Hua and Chunhui Zhang and Hao Wen and Tengfei Liu and Weiqiang Wang},
  booktitle = {The 34th AAAI Conference on Artificial Intelligence, AAAI 2020, New York, NY, USA, February 7-12, 2020},
  pages     = {4667--4674},
  year      = {2020}
}        
</span>
        </pre>
    </table>
    <hr>

    <br>
    <p>Representation Learning Against Adversarial Examples</p>
    <center>
        <span style="font-size:26px" id="hua2019defending"><strong>Defending Against Adversarial Examples via Soft
                Decision Trees Embedding</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#">Yingying Hua</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xindi Gao</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xin Jin</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dan Zeng</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://dl.acm.org/doi/10.1145/3343031.3351012'> Paper
                            [ACM MM 2019]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/Hua2019SDTE.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Convolutional neural networks (CNNs) have shown vulnerable to adversarial examples which contain
            imperceptible perturbations.
            In this paper, we propose an approach to defend against adversarial examples with soft decision trees
            embedding. Firstly, we extract the
            semantic features of adversarial examples with a feature extraction network. Then, a specific soft decision
            tree is trained and embedded
            to select the key semantic features for each feature map from convolutional layers and the selected features
            are fed to a light-weight
            classification network. To this end, we use the probability distributions of each tree node to quantify the
            semantic features. In this
            way, some small perturbations can be effectively removed and the selected features are more discriminative
            in identifying adversarial
            examples. Moreover, the influence of adversarial perturbations on classification can be reduced by migrating
            the interpretability of
            soft decision trees into the black-box neural networks. We conduct experiments to defend the
            state-of-the-art adversarial attacks. The
            experimental results demonstrate that our proposed approach can effectively defend against these attacks and
            improve the robustness
            of deep neural networks.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Hua2019SDTE,
  title     = {Defending Against Adversarial Examples via Soft Decision Trees Embedding},
  author    = {Yingying Hua and Shiming Ge and Xindi Gao and Xin Jin and Dan Zeng},
  booktitle = {The 27th ACM International Conference on Multimedia, MM 2019, Nice, France, October 21-25, 2019},
  pages     = {2106--2114},
  year      = {2019}
}
</span>
        </pre>
    </table>
    <hr>

    <br>
    <p>Multi-Granularity Representation Learning</p>
    <center>
        <span style="font-size:26px" id="wang2020receptive"><strong>Receptive Multi-Granularity Representation for
                Person Re-Identification</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#">Guanshuo Wang</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Yufeng Yuan</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Jiwei Li</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xi Zhou</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/9075365'> Paper
                            [IEEE TIP 2020]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/wang2020RMGL.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            A key for person re-identification is achieving consistent local details for discriminative representation
            across
            variable environments. Current stripe-based feature learning approaches have delivered impressive accuracy,
            but do not make
            a proper trade-off between diversity, locality, and robustness, which easily suffers from part semantic
            inconsistency for the
            conflict between rigid partition and misalignment. This paper proposes a receptive multi-granularity
            learning approach to
            facilitate stripe-based feature learning. This approach performs local partition on the intermediate
            representations to operate
            receptive region ranges, rather than current approaches on input images or output features, thus can enhance
            the representation
            of locality while remaining proper local association. Toward this end, the local partitions are adaptively
            pooled by using
            significance-balanced activations for uniform stripes. Random shifting augmentation is further introduced
            for a higher variance
            of person appearing regions within bounding boxes to ease misalignment. By two-branch network architecture,
            different scales of
            discriminative identity representation can be learned. In this way, our model can provide a more
            comprehensive and efficient
            feature representation without larger model storage costs. Extensive experiments on intra-dataset and
            cross-dataset evaluations
            demonstrate the effectiveness of the proposed approach. Especially, our approach achieves a stateof-the-art
            accuracy of
            96.2%@Rank-1 or 90.0%@mAP on the challenging Market-1501 benchmark.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@article{wang2020RMGL,
  title   = {Receptive Multi-Granularity Representation for Person Re-Identification},
  author  = {Guanshuo Wang and Yufeng Yuan and Jiwei Li and Shiming Ge and Xi Zhou},
  journal = {IEEE Transactions on Image Processing},
  volume  = {29},
  pages   = {6096--6109},
  year    = {2020}
}
</span>
        </pre>
    </table>
    <hr>

    <br>
    <p>Fewer-Shots and Lower-Resolutions: Towards Ultrafast Face Recognition in the Wild</p>
    <center>
        <span style="font-size:26px" id="ge2019fewer"><strong>Fewer-Shots and Lower-Resolutions: Towards Ultrafast Face
                Recognition in
                the
                Wild</strong></span><br>

        <br><br>
        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Shengwei Zhao</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xindi Gao</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Jia Li</a></span> &emsp;
                </td>
            </tr>
        </table>
        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://dl.acm.org/doi/10.1145/3343031.3351082'> Paper [ACM
                            MM 2019]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/GeZG019.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Is it possible to train an effective face recognition model with fewer shots that works efficiently on
            low-resolution faces in the wild? To answer this question, this paper proposes a few-shot knowledge
            distillation approach to learn an ultrafast face recognizer via two steps. In the first step, we initialize
            a simple yet effective face recognition model on synthetic low-resolution faces by distilling knowledge from
            an existing complex model. By removing the redundancies in both face images and the model structure, the
            initial model can provide an ultrafast speed with impressive recognition accuracy. To further adapt this
            model into the wild scenarios with fewer faces per person, the second step refines the model via few-shot
            learning by incorporating a relation module that compares low-resolution query faces with faces in the
            support set. In this manner, the performance of the model can be further enhanced with only fewer
            low-resolution faces in the wild. Experimental results show that the proposed approach performs favorably
            against state-of-the-arts in recognizing low-resolution faces with an extremely low memory of 30KB and runs
            at an ultrafast speed of 1,460 faces per second on CPU or 21,598 faces per second on GPU.
        </p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Ge2019FSLR,
    author    = {Shiming Ge and Shengwei Zhao and Xindi Gao and Jia Li},
    title     = {Fewer-Shots and Lower-Resolutions: Towards Ultrafast Face Recognition in the Wild},
    booktitle = {The 27th ACM International Conference on Multimedia, MM 2019, Nice, France, October 21-25, 2019},
    pages     = {229--237},
    year      = {2019},
}    
</span>
        </pre>
    </table>
    <hr>
    <br>
    <p>We are studying more works about Controllable Representation Learning, including Explainable Representation
        Learning,
        Privacy Preserving Representation Learning, Fair Representation Learning, and Representation learning from
        Multi-source Heterogeneous Data!</p>

</body>

</html>